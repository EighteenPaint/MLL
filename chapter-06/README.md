# SVM    
## 对于数据是否线性可分的思考     
我们可以不断的选出两个特征值进行可视化进行观察是否直观的线性可分    
可以借助信息论熵理论不断的选择属性，然后按照属性对信息增益的大小进行排序，   
选择出影响较大的的属性（当然这也是我的一种想法）
## 超平面  
低一维的平面，需要做到不偏不倚，这是一种哲学  
在SVM中，离超平面的距离的越远，结果就越确定  
## SVM个人描述  
找到一个超平面，使得支持向量最大化，即离超平面最近的点到超平面的距离取得最大值  
疑问： 
* 如何找到支持向量
* 如何使得距离最大:
   * 猜想：距离的最大值就好比逻辑回归的误差值，只不过我们需要这个“误差”最大化，在更新  
   w值上可以考虑与逻辑回归相反的方向修正   
   这里有一个很清晰但可能很多同学没有意识到的问题，为什么我们要不断寻找这个最大值，因为这个超平面需要雨露均沾  
   需要对超平面两边的支撑向量都要得到最大化
   
     
## 对矩阵的思考  
矩阵可以统一有关方程（f = wTx）和函数的运算，并且可以扩展到人类无法感知的高层维度  

## 总结  
SVM底层的数学理论和思想还是有些复杂，在该学习阶段，我们暂时不进入复杂的推导，单纯从应用上看如何应用  
在之后，会有专门的时间深入底层原理

## SVM和线性分类器的区别   
SVM和支持向量机的区别在于SVM对间隔有所要求，而不是仅仅只是分开的数据，值得注意的是，这里的间隔是定义的  
所以在学习的时候对于定义的东西不要过于纠结   
## KKT    
引入KKT是因为，在进行判别时要求,假设分类器正确，应该有yi * （wx + b）> 0,不过在SVM中，一个好的分类器  
应该保证带测点不在间隔内，这个时候即有：yi * （wx + b）> = 1  
在拉格朗日和对偶问题的协助下，可以将最后的数学表达式wx+b变换成由α和  
建议对于SVM的学习，如果只是想看懂的话建议：<西瓜书>+<实战>+<视频>
如果想在深入一点再加一本<最优化理论>     
[SVM视频讲解](https://www.bilibili.com/video/BV1ZE411p73x) 

对于SVM的学习强烈推荐《统计学习方法》，这个里面讲的比较详细  
首先SVM发展历史非常成熟，所以如果单纯从使用的情况下，其实不难，在大数据量情况下，依然是深度学习的未来  
SVM算法相对KNN这种需要保留几乎所有的训练集的算法有有优势，因为svm只需要保存支持向量即可