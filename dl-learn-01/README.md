# 摘要
1. 神经网络本质是对数据的变换，在我理解，是对数据的压缩与特征提取
2. 机器学习整体是一个偏实践而非偏理论的，也就意味着，有些知识点实践验证是可行的，但是没法使用数学工具进行解释
3. 深度学习的目的是获取一系列权重参数，方法就是通过不断反馈损失函数的值去修正当前可能不好的参数，直到找到损失函数最小的参数集
学习就是一个不断假设正确和否定并修正已有认知的重复过程
4. 深度学习是一种题海战术，必须在有足够题量的前提下进行训练，也就是海量的几乎无限的数据，如果数据不是特别多，那么基于统计学机器学习可以做的很好
5. 机器学习模型分类：
    1. 概率模型(朴素贝叶斯，隐马尔可夫,因为概率相关定理非常多也就意味着相应的工具也多，更容易解决问题，就好像所有的图形只要可以转化为无数个三角形，问题就会迎刃而解，建议重点研究)
    2. 回归模型(逻辑回归，线性回归，只要通过拟合的方式实现，这种方式其实比较成熟了)
    3. 神经网络模型(BP，CNN)
    4. 无模型学习（KNN，K-means）
    5. 超平面法（决策树，SVM）
    6. 集成学习
6. 深度学习与机器学习简单集成不同的是，深度学习每层是共同学习，不是依次学习
7. 要想在如今的应用机器学习中取得成功，你应该熟悉这两种技术：梯度提升机（XGBOOST），用于浅层学习问题；深度学习，用于感知问题。用术语来说，你需要熟悉 XGBoost 和 Keras，它们是目
前主宰 Kaggle 竞赛的两个库。传统机器学习可以很好处理的数据问题（分类，聚类，关联，推荐），深度学习擅长处理感知问题(视觉，听觉，情感，语言)
8. 矩阵在表示表示上是2维，只有行和列，而张量可以不断嵌套，将多个 3D 张量组合成一个数组，可以创建一个 4D 张量，以此类推。深度学习处理的一般
是 0D 到 4D 的张量，但处理视频数据时可能会遇到 5D 张量
9. 对于张量的理解，从形状上看从左往右看，从意义上看位置对应，最后两个元素相当于坐标轴，确定位置，在往右看就是每个位置有几个信息。比如一张图片每个像素点需要三个信息（R,G,B）,就需要3个128*128二维矩阵，与之位置进行对于
数据量<属性<Y<X   对应（samples，color,height,width）
10. 使用动量优化来避免局部最优解，其实如果动量选择的太小，依然有可能出现局部最优解？？？局部最优解真的可以避免么？无论采取怎样的措施都有可能会出现吧？对于单纯使用梯度下降的算法，忽略了上次的状态，加入动量，可以结合上一次的状态进行训练，往往可以更快的收敛。但是如果选择不当，依然有可能达到局部最优而无法全局最优
11. 反向传播算法:本质是链式求导法则，每一层递进，直观上还是比较好理解的