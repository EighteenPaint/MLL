## 回归分析  
在数学里面，有些名字不代表任何含义，只有意义，让我想起了几何分布跟几何没有半毛钱关系

## 对于最小二乘法的思考  
1. 每个数据点误差最小必然可以得出最终的和最小，而最终和最下却不一定能够保证每个点的误差最小，虽然不是充分必要的条件  
但是我们一定需要怎么严格么？难道就没有一种理想的情况使得充分条件变成充分必要条件，这就是我对最小二乘法的理解与思考  
其实这也就是我们在进行算法设计的时候，经常会考虑算法失效的情况，而这种情况就是让充分必要条件失效的原因，比如离群点  
异常点的处理
2. 为了更好的拟合曲线，实现知道数据的样子就显得格外重要，这也就是我之前的说的，在真正开始处理数据之前，先进行可视化是一个非常好的选择
甚至我会把其列为自己处理问题的原则

## 局部加权线性回归  
在直观上收紧了数据，让原本距离远的点，变得近一些

## 再看算法推导时我们应该注意什么  
1. 哪些是已知的：比如数据集往往是已知的  
2. 哪些是需要通过经验指定的，比如核函数选取，SVM的C值，迭代次数
3. 不断迭代过程中是相关的参数是如何更新的

## 局部加权线性回归的一些问题  
局部加权回归每次需要测试数据结合原数据集进行权重计算，这一点明显在性能上会有缺陷

## 解释线性回归的系数  
系数的大小可以解释为某个属性的的重要程度，也就是说我们从一大堆特征中找到最具有代表性的特征

## Jupyter  
一个web版交互式页面，交互式比把程序编好在执行肯定是有一定好处的，直观

## 总结  
1. 必须可视化
2. 必须标准化

## Q问题栏  
1. 为什么说缩减方法可以去掉不重要的参数，他是如何判断哪些参数不重要，而且又是怎样去除的呢
2. 缩减数据能更好的理解数据，这里的理解数据是什么意思
3. 过拟合是怎么发生的，不是应该是一个直线么，而且也没有高次项，难道是变成分段函数了
