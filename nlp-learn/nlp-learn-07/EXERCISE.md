1. IOB格式分类标注标识符为I、 O和 B。三个标签为什么是必要的？如果我们只使用
I和 O标记会造成什么问题？
2. 写一个标记模式匹配包含复数中心名词在内的名词短语， 如many/JJ researchers
/NNS, two/CD weeks/NNS, both/DT new/JJ positions/NNS。通过泛化处理单
数名词短语的标记模式，尝试做这个。
3. 选择CoNLL-2000分块语料库中三种块类型之一。查看这些数据， 并尝试观察组成这
种类型的块的 POS标记序列的任一模式。 使用正则表达式分块器nltk.RegexpParser
开发一个简单的分块器。讨论任何难以可靠分块的标记序列。
4. 块的早期定义是出现在缝隙之间的材料。开发一个分块器以将完整的句子作为一个单
独的块开始， 然后其余的工作完全由加缝隙完成。 在你自己的应用程序的帮助下， 确定
哪些标记（ 或标记序列）最有可能组成缝隙。 相对于完全基于块规则的分块器比较这种
方法的性能和易用性。
5. 写一个标记模式，涵盖包含动名词在内的名词短语，如the/DT receiving/VBG e
nd/NN, assistant/NN managing/VBG editor/NN。将这些模式加入到 grammar，
每行一个。用自己设计的一些已标注的句子，测试你的工作。
6. 写一个或多个标记模式处理有连接词的名词短语， 如： July/NNP and/CC August
/NNP， all/DT your/PRP$ managers/NNS and/CC supervisors/NNS， compa
ny/NN courts/NNS and/CC adjudicators/NNS。
7. 用任何你之前已经开发的分块器执行下列评估任务。（请注意，大多数分块语料库包
含一些内部的不一致，以至于任何合理的基于规则的方法都将产生错误。）
a. 在来自分块语料库的 100个句子上评估你的分块器， 报告精度、 召回率和F量度。
b. 使用 chunkscore.missed()和 chunkscore.incorrect()方法识别你的分块器的
错误，并讨论它。
c. 与本章的评估部分讨论的基准分块器比较你的分块器的性能。
8. ◑使用基于正则表达式的块语法RegexpChunk， 为 CoNLL分块语料库中块类型中的
一个开发一个分块器。使用分块、加缝隙、合并或拆分规则的任意组合。
9. ◑有时一个词的标注不正确，例如12/CD or/CC so/RB cases/VBZ的中心名词。
替代手工校正标注器的输出， 好的分块器使用标注器的错误输出也能运作。查找使用不
正确的标记正确为名词短语分块的其他例子。
10. ◑bigram分块器的准确性得分约为 90％。研究它的错误，并试图找出它为什么不能获
得 100％的准确率。实验trigram分块。你能够在提高性能吗？
11. ●在IOB块标注上应用 n-gram和Brill标注方法。不是给词分配 POS标记，在这里我
们给 POS标记分配IOB标记。例如：如果标记 DT（限定符）经常出现在一个块的开
头， 它会被标注为B（ begin）。 相对于本章中讲到的正则表达式分块方法， 评估这些分
块方法的性能。
12. ●在第5章我们看到通过查找有歧义的n-grams，即在训练数据中有多种可能的方式标
注的 n-grams， 可以得到标注性能的上限。应用同样的方法来确定一个 n-gram分块器的
上限。
13. ●挑选CoNLL分块语料库中三种块类型之一。写一个函数为你选择的类型做以下任务：
a. 列出与此块类型的每个实例一起出现的所有标记序列。
b. 计数每个标记序列的频率， 并产生一个按频率减少的顺序排列的列表； 每行要包含
一个整数（频率）和一个标记序列。
c. 检查高频标记序列。使用这些作为开发一个更好的分块器的基础。
14. ●在评估一节中提到的基准分块器往往会产生比它应该产生的块更大的块。例如： 短语
[every/DT time/NN] [she/PRP] sees/VBZ [a/DT newspaper/NN]包含两个
连续的块， 我们的基准分块器不正确地将前两个结合： [every/DT time/NN she/P
RP]。 写一个程序， 找出这些通常出现在一个块的开头的块内部的标记有哪些， 然后设
计一个或多个规则分裂这些块。 将这些与现有的基准分块器组合， 重新评估它， 看看你
是否已经发现了一个改进的基准。
15. ●开发一个NP分块器，转换POS标注文本为元组的一个链表，其中每个元组由一个
后面跟一个名词短语和介词的动词组成，如： the little cat sat on the mat becomes ('s
at', 'on', 'NP')...
16. ●宾州树库样例包含一部分已标注的《 华尔街日报》 文本， 已经按名词短语分块。 格式
使用方括号，我们已经在本章遇到它了几次。语料可以使用for sent in nltk.corpus.
treebank_chunk.chunked_sents(fileid)来访问。这些都是平坦的树，正如我们使
用 nltk.corpus.conll2000.chunked_sents()得到的一样。
a. 函数 nltk.tree.pprint()和 nltk.chunk.tree2conllstr()可以用来从一棵树创建
树库和 IOB字符串。 写函数 chunk2brackets()和chunk2iob()， 以一个单独的
块树为它们唯一的参数，返回所需的多行字符串表示。
b. 写命令行转换工具bracket2iob.py和iob2bracket.py，（ 分别） 读取树库或Co
NLL格式的一个文件， 将它转换为其他格式。（ 从NLTK语料库获得一些原始的树
库或 CoNLL数据， 保存到一个文件， 然后使用for line in open(filename)从 P
ython访问它。）
17. 一个n-gram分块器可以使用除当前词性标记和n-1个前面的块的标记以外其他信息。
调查其他的上下文模型， 如 n-1个前面的词性标记， 或一个写前面块标记连同前面和后
面的词性标记的组合。
18. 思考一个n-gram标注器使用临近的标记的方式。现在观察一个分块器可能如何重新
使用这个序列信息。 例如： 这两个任务将使用名词往往跟在形容词后面（ 英文中） 的信
息。 这会出现相同的信息被保存在两个地方的情况。 随着规则集规模增长， 这会成为一
个问题吗？如果是，推测可能会解决这个问题的任何方式